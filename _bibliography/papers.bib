---
---

@article{dewaskar2023robustifying,
  title={Robustifying likelihoods by optimistically re-weighting data},
  author={M. Dewaskar and C. Tosh and J. Knoblauch and D. B. Dunson},
  journal={arXiv preprint arXiv:2303.10525},
  arxiv={2303.10525},
  year={2023}
}

@article{tosh2022targeted,
  title={Targeted active learning for probabilistic models},
  author={C. Tosh and M. Tec and W. Tansey},
  journal={arXiv preprint arXiv:2210.12122},
  arxiv={2210.12122},
  year={2022}
}

@inproceedings{tosh22simple,
  title = 	 {Simple and near-optimal algorithms for hidden stratification and multi-group learning},
  author =       {C. Tosh and D. Hsu},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {21633--21657},
  year = 	 {2022},
  pdf = 	 {https://proceedings.mlr.press/v162/tosh22a/tosh22a.pdf},
  html = 	 {https://proceedings.mlr.press/v162/tosh22a.html},
  arxiv = {2112.12181},
  abstract = 	 {Multi-group agnostic learning is a formal learning criterion that is concerned with the conditional risks of predictors within subgroups of a population. The criterion addresses recent practical concerns such as subgroup fairness and hidden stratification. This paper studies the structure of solutions to the multi-group learning problem, and provides simple and near-optimal algorithms for the learning problem.},
  selected={true}
}


@article{tansey22bayesian,
author = {W. Tansey and C. Tosh and D. M. Blei},
title = {{A Bayesian model of dose-response for cancer drug studies}},
volume = {16},
journal = {The Annals of Applied Statistics},
number = {2},
pages = {680 -- 705},
arxiv = {1906.04072},
abstract = {Exploratory cancer drug studies test multiple tumor cell lines against multiple candidate drugs. The goal in each paired (cell line, drug) experiment is to map out the dose-response curve of the cell line as the dose level of the drug increases. We propose Bayesian tensor filtering (BTF), a hierarchical Bayesian model for dose-response modeling in multisample, multitreatment cancer drug studies. BTF uses low-dimensional embeddings to share statistical strength between similar drugs and similar cell lines. Structured shrinkage priors in BTF encourage smoothness in the dose-response curves while remaining adaptive to sharp jumps when the data call for it. We focus on a pair of cancer drug studies exhibiting a particular pathology in their experimental design, leading us to a nonconjugate monotone mixture-of-gammas likelihood. To perform posterior inference, we develop a variant of the elliptical slice sampling algorithm for sampling from linearly-constrained multivariate normal priors with nonconjugate likelihoods. In benchmarks, BTF outperforms state-of-the-art methods for covariance regression and dynamic Poisson matrix factorization. On the two cancer drug studies, BTF outperforms the current standard approach in biology and reveals potential new biomarkers of drug sensitivity in cancer. Code is available at https://github.com/tansey/functionalmf.},
keywords = {constrained inference, Dose-response, matrix factorization, slice sampling, Trend filtering},
year = {2022}
}


@inproceedings{simchowitz21bayesian,
 author = {M. Simchowitz and C. Tosh and A. Krishnamurthy and D. Hsu and T. Lykouris and M. Dudik and R. Schapire},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Bayesian decision-making under misspecified priors with applications to meta-learning},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2021/file/ddcbe25988981920c872c1787382f04d-Paper.pdf},
 volume = {34},
 arxiv = {2107.01509},
 year = {2021}
}

@article{tosh2021piranha,
  title={The piranha problem: Large effects swimming in a small pond},
  author={C. Tosh and P. Greengard and B. Goodrich and A. Gelman and A. Vehtari and D. Hsu},
  journal={arXiv preprint arXiv:2105.13445},
  arxiv = {2105.13445},
  year={2021}
}


@inproceedings{tosh21contrastive,
  title = 	 {Contrastive learning, multi-view redundancy, and linear models},
  author =       {C. Tosh and A. Krishnamurthy and D. Hsu},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {1179--1206},
  year = 	 {2021},
  pdf = 	 {http://proceedings.mlr.press/v132/tosh21a/tosh21a.pdf},
  html = 	 {https://proceedings.mlr.press/v132/tosh21a.html},
  arxiv = {2008.10150},
  abstract = 	 {Self-supervised learning is an empirically successful approach to unsupervised learning based on creating artificial supervised learning problems. A popular self-supervised approach to representation learning is contrastive learning, which leverages naturally occurring pairs of similar and dissimilar data points, or multiple views of the same data. This work provides a theoretical analysis of contrastive learning in the multi-view setting, where two views of each datum are available. The main result is that linear functions of the learned representations are nearly optimal on downstream prediction tasks whenever the two views provide redundant information about the label.},
  selected={true}
}


@article{tosh2021contrastivetopic,
  title={Contrastive estimation reveals topic posterior information to linear models},
  author={C. Tosh and A. Krishnamurthy and D. Hsu},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={12883--12913},
  arxiv = {2003.02234},
  year={2021}
}

@inproceedings{tosh2020diameter,
  title = 	 {Diameter-based interactive structure discovery},
  author =       {C. Tosh and D. Hsu},
  booktitle = 	 {Proceedings of the Twenty-Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {580--590},
  year = 	 {2020},
  pdf = 	 {http://proceedings.mlr.press/v108/tosh20a/tosh20a.pdf},
  html = 	 {https://proceedings.mlr.press/v108/tosh20a.html},
  arxiv = {1906.02101},
  abstract = 	 {We introduce interactive structure discovery, a generic framework that encompasses many interactive learning settings, including active learning, top-k item identification, interactive drug discovery, and others. We adapt a recently developed active learning algorithm of Tosh and Dasgupta for interactive structure discovery, and show that the new algorithm can be made noise-tolerant and enjoys favorable query complexity bounds.}
}


@article{dasgupta2020expressivity,
  title={Expressivity of expand-and-sparsify representations},
  author={S. Dasgupta and C. Tosh},
  journal={arXiv preprint arXiv:2006.03741},
  arxiv = {2006.03741},
  year={2020}
}


@inproceedings{tosh2019relatve,
  title = {The relative complexity of maximum likelihood estimation, {MAP} estimation, and sampling},
  author =    {C. Tosh and S. Dasgupta},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {2993--3035},
  year = 	 {2019},
  pdf = 	 {http://proceedings.mlr.press/v99/tosh19a/tosh19a.pdf},
  html = 	 {https://proceedings.mlr.press/v99/tosh19a.html},
  abstract = 	 {We prove that, for a broad range of problems, maximum-a-posteriori (MAP) estimation and approximate sampling of the posterior are at least as computationally difficult as maximum-likelihood (ML) estimation. By way of illustration, we show how hardness results for ML estimation of mixtures of Gaussians and topic models carry over to MAP estimation and approximate sampling under commonly used priors.}
}

@article{dasgupta2019interactive,
  title={Interactive topic modeling with anchor words},
  author={S. Dasgupta and S, Poulis and C. Tosh},
  journal={arXiv preprint arXiv:1907.04919},
  arxiv = {1907.04919},
  year={2019}
}

@inproceedings{tosh2018interactive,
 author = {C. Tosh and S. Dasgupta},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Interactive structure learning with structural {Query-by-Committee}},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2018/file/08c5433a60135c32e34f46a71175850c-Paper.pdf},
 arxiv = {1803.06586},
 year = {2018}
}


@inproceedings{tosh2017diameter,
  title = 	 {Diameter-based active learning},
  author =       {C. Tosh and S. Dasgupta},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3444--3452},
  year = 	 {2017},
  pdf = 	 {http://proceedings.mlr.press/v70/tosh17a/tosh17a.pdf},
  html = 	 {https://proceedings.mlr.press/v70/tosh17a.html},
  abstract = 	 {To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance.},
  arxiv = {1702.08553},
  selected = {true}
}

@article{tosh2017maximum,
  title={Maximum likelihood estimation for mixtures of spherical {G}aussians is {NP}-hard.},
  author={C. Tosh and S. Dasgupta},
  journal={Journal of Machine Learning Research},
  volume={18},
  pages={175--1},
  year={2017}
}


@inproceedings{tosh2016mixing,
  title = 	 {Mixing rates for the alternating {G}ibbs sampler over restricted {B}oltzmann machines and friends},
  author = 	 {C. Tosh},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {840--849},
  year = 	 {2016},
  pdf = 	 {http://proceedings.mlr.press/v48/tosh16.pdf},
  html = 	 {https://proceedings.mlr.press/v48/tosh16.html},
  abstract = 	 {Alternating Gibbs sampling is a modification of classical Gibbs sampling where several variables are simultaneously sampled from their joint conditional distribution. In this work, we investigate the mixing rate of alternating Gibbs sampling with a particular emphasis on Restricted Boltzmann Machines (RBMs) and variants.}
}


@inproceedings{tosh2014lower,
  title = 	 {Lower bounds for the {G}ibbs Sampler over mixtures of {G}aussians},
  author = 	 {C. Tosh and S. Dasgupta},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1467--1475},
  year = 	 {2014},
  pdf = 	 {http://proceedings.mlr.press/v32/tosh14.pdf},
  html = 	 {https://proceedings.mlr.press/v32/tosh14.html},
  abstract = 	 {The mixing time of a Markov chain is the minimum time t necessary for the total variation distance between the distribution of the Markov chain’s current state X_t and its stationary distribution to fall below some ε&gt; 0. In this paper, we present lower bounds for the mixing time of the Gibbs sampler over Gaussian mixture models with Dirichlet priors.}
}
